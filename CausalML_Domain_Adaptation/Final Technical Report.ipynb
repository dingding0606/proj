{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4439c8",
   "metadata": {},
   "source": [
    "# Technical Report: CSCI Independent Study\n",
    "2023 Spring Semester\n",
    "\n",
    "Student: Yufeng Wu\n",
    "\n",
    "Advisor: Rohit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd8f44e",
   "metadata": {},
   "source": [
    "## I. Proposed Research Contribution\n",
    "\n",
    "We propose a new method that utilizes information derived from causal Directed Acyclic Graphs (DAGs) to identify the stable components of a predictive model when faced with distribution shifts. This method enables us to selectively retrain only the necessary parts of the model using data from the altered environment. Our primary objective is to outperform the baseline model in the shifted environment, surpassing the results that would be achieved by training a completely new model from scratch using the limited data available from the shifted environment.\n",
    "\n",
    "\n",
    "## II. Problem Set Up\n",
    "\n",
    "The goal of this research is to propose a novel method of achieving domain adaptation when facing distribution shift between the source and the target environment.  \n",
    "\n",
    "Suppose we are given a causal DAG which involves Y (the target variable), V (a set of predictors of Y, all of which are causal ancesters of Y), and possibly U (a set of unobserved variables in the DAG). We further assume that the DAG factorization across the two environments are the same, but some of the terms of the factorization may be different.\n",
    "\n",
    "We further assume that the information of which edge has shifted is given to us. That is, we do not consider how to detect distribution shift in this research. \n",
    "\n",
    "We have a large amount of training data available to us in the training environment ($N$). In out problem, we consider the case where we have a few data points ($M$) in the shifted environment, where $N >> M$. \n",
    "\n",
    "The task is to train a predictive model that achieves the highest accuracy in the shifted environment.\n",
    "\n",
    "## III. Background\n",
    "\n",
    "#### Definition of stability and distribution shift (Subbaswamy et al. in \"unifying framework\"):\n",
    "We graphically charaterize instability in terms of edges in the graph of the data generating process. Assume that there is a set of environments such that a predictive problem is mapped to the same graph structure $G$. However, each environment is a different instantiation of that graph such that certain mechanisms differ. Thus, the factorization of the data distribution is the same in each environment, but terms in the factorization corresponding to shifts will vary across environments. \n",
    "\n",
    "\n",
    "#### Defintion of unstable edge (modified from Subbaswamy et al. in \"unifying framework\"):\n",
    "An edge $X \\to Y$ is said to be unstable if, in 2 different environments of $G$, the distribution $P(Y(x', V(x)) - Y(x))$ changes, where:\n",
    "\n",
    "- $V = pa(Y) \\setminus X$\n",
    "\n",
    "- $Y(x)$ is the counterfactual value of $Y$ had $X$ been $x$\n",
    "\n",
    "- $Y(x', V(x))$ is the counterfactual value of $Y$ had $X$ been $x'$ and had $V$ been counterfactually generated under $X = x$.\n",
    "\n",
    "#### Using unstable edge to model common types of distribution shifts: \n",
    "\n",
    "The above definition is able to model:\n",
    "\n",
    "1. Edge-strength shift: An edge strength shift in edge $X \\to Y$ corresponds to a change in the natural direct effect: for $V = pa(Y) \\setminus X$, $E(Y(x', V(x)) - Y(x))$ changes. \n",
    "\n",
    "Notice that when this expected value changes, the distribution $P(Y(x', V(x)) - Y(x))$ necessarily changes too. Thus, our definition of unstable edge is able to capture edge-strength shift.\n",
    "\n",
    "2. Mechanism shift: A shift in the mechanism generating a variable $Y$ corresponds to arbitrary changes in the distribution $P(Y∣pa(Y))$. Suppose $Y = f(pa(Y)) + \\epsilon$ is the deterministic data generating process of $Y$ given its parents, where $f$ is a function representing the relationship between $Y$ and its parents $pa(Y)$, and $\\epsilon$ is an error term that is independent of $pa(Y)$.\n",
    "\n",
    "Now, let's consider a mechanism shift in the edges $pa(Y) \\to Y$. If the function $f$ changes to $f'$, the data-generating process becomes $Y = f'(pa(Y)) + \\epsilon$. This change in the mechanism affects the distribution $P(Y∣pa(Y))$. \n",
    "\n",
    "Suppose $X \\in pa(Y)$ and $V = pa(Y) \\setminus X$. Let's examine the edge $X\\to Y$ first. Under the counterfactual scenarios $Y(x)$ and $Y(x', V(x))$, the values of $Y$ are determined by the respective functions $f$ and $f'$ before and after the mechanism shift, which means the difference in counterfactual values, $Y(x', V(x)) - Y(x)$, will be influenced by this mechanism shift.\n",
    "\n",
    "Since the mechanism shift can lead to arbitrary changes in the distribution $P(Y∣pa(Y))$, it can also cause changes in the distribution $P(Y(x', V(x)) - Y(x))$. Thus, our definition of unstable edge is able to capture the case when mechanism shift happens. \n",
    "\n",
    "In fact, when there are arbitrary changes in the distribution $P(Y∣pa(Y))$, it is likely that, for each $X_i \\in pa(Y)$ and $V_i = pa(Y) \\setminus X_i$, the distributions $P(Y(x_i', V_i(x_i))) - Y(x_i))$ changes. Thus, we should mark all edges coming into $Y$ as unstable.\n",
    "\n",
    "#### Generalized Additive Model\n",
    "\n",
    "A Generalized Additive Model (GAM) takes the form\n",
    "\n",
    "$ g(\\operatorname {E}(Y))=\\beta _{0}+f_{1}(x_{1})+f_{2}(x_{2})+\\cdots +f_{m}(x_{m}) $\n",
    "\n",
    "where $f_i$ can be arbitrarily complex functions and $x_i$'s are the predictors of $Y$. \n",
    "\n",
    "The link function $g(\\cdot)$ serves the following purposes:\n",
    "1. it allows for the modeling of relationships between the predictor variables and the response variable that may not be linear, by transforming the expected value of the response variable.\n",
    "2. it ensures that the expected value of the response variable lies within the permissible range, especially for response variables with bounded ranges (e.g., probabilities must be between 0 and 1).\n",
    "\n",
    "Some common link functions used in GAMs include:\n",
    "\n",
    "- Identity link: $g(E(Y)) = E(Y)$, used for continuous response variables in linear regression models.\n",
    "- Logit link: $g(E(Y)) = \\log\\left(\\frac{E(Y)}{1 - E(Y)}\\right)$, used for binary response variables in logistic regression models.\n",
    "- Log link: $g(E(Y)) = \\log(E(Y))$, used for count or rate data in Poisson and negative binomial regression models.\n",
    "\n",
    "The choice of link function depends on the distribution of the response variable and the desired relationship between the predictor variables and the response variable.\n",
    "\n",
    "\n",
    "## IV. Finding stable component in a GAM using d-seperation rules\n",
    "\n",
    "Suppose we are given a set of observed causal ancestors of $Y$, the causal relationship between them and possibly some unobserved variables, and which edges in the DAG are unstable across the source and target environments. Now, if $Y$ can be modeled using a GAM, how can we determine which variables are stable across the two environments?\n",
    "\n",
    "#### Definition of a stable predictor\n",
    "\n",
    "A predictor is stable if the association between such predictor and $Y$ are constant across two environments, given all the other variables that are also in the model. In other words, a predictor is stable if the portion of its effect to $Y$ that does not go through any other predictors is constant. **Mathematically ??**\n",
    "\n",
    "#### Definition of a stable component\n",
    "\n",
    "A stable component of a GAM across the source and target environment is a collection/set of stable predictors. \n",
    "\n",
    "#### Finding the stable component\n",
    "\n",
    "The following algorithm outputs the set of predictors that forms a stable component of a GAM. \n",
    "\n",
    "Step 1: draw a square around all the predictors in the model to indicate that, when training a predictive model on $Y$ using the predictors $X_1, X_2, ..., X_n$, it is analogous to conditioning on them in the DAG. This is because, in the context of a DAG, conditioning on variables means considering their values in the analysis, which is what the machine learning model does when using them as predictors.\n",
    "\n",
    "Step 2: for each predictor in the ML model, check all the paths from that variable to Y, including both front-door and back-door paths. A predictor is stable if there is no unstable path from that predictor to $Y$, when conditioning on the rest of the predictors. \n",
    "\n",
    "The set of all the stable predictors found in step 2 is the stable component that is directly transferable from the source to the target environment.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Suppose we are given the following data generating process, where $U1$ and $U2$ are unobserved and unstable edges across the two environments are colored red. \n",
    "\n",
    "<img src=\"example_1_DAG.png\" alt=\"pic\" width=\"30%\">\n",
    "\n",
    "To determine a stable component of a GAM that predicts $Y$ using $X_1, X_2, X_3$, we need to draw a square around all the predictors, see below. By d-separation, when learning a ML model that predicts $Y$ using $X_1, X_2, X_3$, the association between $X_1$ and $Y$ is learned through the path $X_1 \\to U_2 \\to Y$ only, because the other flow of association from $X_1$ to $Y$ that goes through $X_2$ is blocked since $X_2$ is being \"conditioned on\" during the model training process. Note: this is because -- when learning the association between $X_1$ and $Y$, we're essentially asking the question: how does $Y$ change depending on $X_1$, given all else constant. The \"given all else constant\" part is equivalent to \"conditioning\" in causal terms, because we are fixing $X_2$ and $X_3$ to some specific values when learning the relationship between $X_1$ and $Y$. \n",
    "\n",
    "**Question: is that still true when $X_1$ and $X_2$ interact?**\n",
    "\n",
    "<img src=\"example_1_conditioned.png\" alt=\"pic\" width=\"30%\">\n",
    "\n",
    "Therefore, if we learned a model $g(\\operatorname {E}(Y))=\\beta _{0}+f_{1}(X_{1})+f_{2}(X_{2}) +f_{3}(X_{3}) $ that is able to predict $Y$ in the source environment, then $f_1(X_1)$ and $f_3(X_3)$ are stable predictors across the two environments whereas $f_2(X_2)$ is not stable because there is an unstable path between $X_2$ and $Y$. So, the association learned in the source environment $f_2(X_2)$ is not transportable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4cda5",
   "metadata": {},
   "source": [
    "## V. Identifying and dealing with interaction terms\n",
    "\n",
    "In the above example, when $X_1$ and $X_2$ interact, then we are not able to accurately model $Y$ with a fully additive model. When $X_1$ and $X_2$ interact, the association between $X_1, X_2$ and $Y$ can no longer be modeled accurately using $Y = f_{1}(X_{1})+f_{2}(X_{2}) + \\text{intercept}$. Instead, we have to model such association using $Y = f(X_1, X_2) + \\text{intercept}$.\n",
    "\n",
    "Therefore, we need to add one more criteria on the definition of \"stable component\" of a GAM across the source and target environments. Specifically, we need to require that **non of the predictors in the stable component interact with the predictors in the unstable component** (defined by the set difference between all predictors and the predictors in the stable componenet).\n",
    "\n",
    "THOUGHTS: MAYBE WE CAN JUST BORROW AN EXISTING STATISTICAL TEST TO DETERMINE WHETHER THERE EXISTS AN INTERACTION TERM OR NOT.\n",
    "\n",
    "#### Algorithm for finding the largest subset of stable component (as determined by the d-sep rules) that does not interact with the unstable component:\n",
    "\n",
    "Step 1: create two neural nets, one for the initially identified stable component (call it $N_a$) and another for the unstable component (call it $N_b$). Let the outputs of the two neural nets be $Y_1$ and $Y_2$ respectively. Let the prediction $\\hat{Y} = g(Y_1 + Y_2)$ where $g$ is some pre-defined link function. \n",
    "\n",
    "Step 2: co-train the two models with the target of minimizing the difference (e.g. MSE / cross-entropy loss) between $\\hat{Y}$ and the ground truth $Y$. Stop until training converges.\n",
    "\n",
    "Step 3:\n",
    "\n",
    "```\n",
    "While True:\n",
    "\n",
    "    interaction_detected = False\n",
    "    \n",
    "    for p in N_a.predictors():\n",
    "        create connections from p to nodes in the second layer of N_b\n",
    "        continue training until convergence\n",
    "        \n",
    "        if at least one of these newly created edge has weights > threshold:\n",
    "            # this means p interact with at least one predictor in N_b\n",
    "            interaction_detected = True\n",
    "            \n",
    "            remove p from N_a\n",
    "            add p as a new predictor in N_b\n",
    "            retrain N_a and N_b simultaneously until convergence\n",
    "            \n",
    "            break out of the for loop\n",
    "     \n",
    "     if interaction_detected == False:\n",
    "         break out of the while loop\n",
    "```\n",
    "\n",
    "Now, the predictors left in $N_a$ is the stable component that does not interact with:\n",
    "1. any predictor in the initially detected unstable component\n",
    "2. any predictor that interacts with predictors in the initially detected unstable component\n",
    "\n",
    "#### Note: \n",
    "\n",
    "This paper on prior NN might be relevant: http://proceedings.mlr.press/v119/rieger20a/rieger20a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7fb6cc",
   "metadata": {},
   "source": [
    "## VI. Experiment 1: my own DGP, 3 predictors, no interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f4e9b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "dcf9f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def train_single_net(net, inputs, targets, num_epochs=1000):\n",
    "    '''\n",
    "    Trains a single neural network using the provided inputs and targets.\n",
    "\n",
    "    Parameters:\n",
    "    net (nn.Module): The neural network to train.\n",
    "    inputs (torch.Tensor): The input features tensor of shape (n_samples, n_features).\n",
    "    targets (torch.Tensor): The ground truth target variable tensor of shape (n_samples, 1).\n",
    "    num_epochs (int, optional): The number of epochs to train the neural network. Default is 1000.\n",
    "    learning_rate (float, optional): The learning rate for the optimizer. Default is 0.001.\n",
    "\n",
    "    Returns:\n",
    "    nn.Module: The trained neural network.\n",
    "    '''\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def co_train(net_a, net_b, inputs, targets, net_a_predictors, net_b_predictors, link_function, num_epochs=1000):\n",
    "    '''\n",
    "    Co-trains two neural networks, net_a and net_b, with the target of minimizing the \n",
    "    loss between the predicted output (Y_hat = link_function(net_a_output + net_b_output)) \n",
    "    and the ground truth.\n",
    "\n",
    "    Parameters:\n",
    "    net_a (nn.Module): The neural network representing the stable component.\n",
    "    net_b (nn.Module): The neural network representing the unstable component.\n",
    "    inputs (torch.Tensor): The input features tensor of shape (n_samples, n_features).\n",
    "    targets (torch.Tensor): The ground truth target variable tensor of shape (n_samples, 1).\n",
    "    net_a_predictors (list): The indices of input features in 'inputs' that are part of net_a.\n",
    "    link_function (callable): A function that combines the outputs of net_a and net_b.\n",
    "    num_epochs (int, optional): The number of epochs to train the neural networks. Default is 10000.\n",
    "\n",
    "    Returns:\n",
    "    tuple: The co-trained neural networks net_a and net_b.\n",
    "    '''\n",
    "    \n",
    "    optimizer_a = optim.Adam(net_a.parameters())\n",
    "    optimizer_b = optim.Adam(net_b.parameters())\n",
    "    criterion = nn.MSELoss() # add a loss to prior?!!!?\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        outputs_a = net_a(inputs[:, net_a_predictors])\n",
    "        outputs_b = net_b(inputs[:, net_b_predictors])\n",
    "        y_hat = link_function(outputs_a + outputs_b)\n",
    "\n",
    "        loss = criterion(y_hat, targets)\n",
    "        \n",
    "        optimizer_a.zero_grad()\n",
    "        optimizer_b.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer_a.step()\n",
    "        optimizer_b.step()\n",
    "\n",
    "    return net_a, net_b\n",
    "\n",
    "# This is probabily incorrect. Need to test it. \n",
    "def largest_stable_subset(net_a, \n",
    "                          net_b, \n",
    "                          inputs, \n",
    "                          targets, \n",
    "                          net_a_predictors, \n",
    "                          net_b_predictors, \n",
    "                          link_function, \n",
    "                          threshold, \n",
    "                          num_epochs):\n",
    "    \n",
    "    while True:\n",
    "        interaction_detected = False\n",
    "        net_a_copy = copy.deepcopy(net_a)\n",
    "        net_b_copy = copy.deepcopy(net_b)\n",
    "\n",
    "        for predictor in net_a_predictors:\n",
    "            # Append the new predictor/feature index to net_b_predictors\n",
    "            net_b_predictors.append(predictor)\n",
    "            \n",
    "            # Create a copy of the original net_b.layer1.weight\n",
    "            original_weight = net_b.layer1.weight.clone()\n",
    "\n",
    "            # Add a column of zeros for the new connection\n",
    "            zero_column = torch.zeros(net_b.layer1.weight.size(0), 1)\n",
    "            new_weight = nn.Parameter(torch.cat((zero_column, original_weight), dim=1))\n",
    "            \n",
    "            # Update the weights between the 1st and 2nd layer of net_b, \n",
    "            # since net_b now has one more input neuron\n",
    "            input_dim = net_b.layer1.in_features + 1\n",
    "            output_dim = net_b.layer1.out_features\n",
    "            net_b.layer1 = nn.Linear(input_dim, output_dim)\n",
    "            net_b.layer1.weight = new_weight\n",
    "            \n",
    "            # Continue training until convergence\n",
    "            net_a, net_b = co_train(net_a=net_a, \n",
    "                                    net_b=net_b, \n",
    "                                    inputs=inputs, \n",
    "                                    targets=targets, \n",
    "                                    net_a_predictors=net_a_predictors,\n",
    "                                    net_b_predictors=net_b_predictors,\n",
    "                                    link_function=link_function,\n",
    "                                    num_epochs=num_epochs)\n",
    "            \n",
    "            # The newly added connects are the weights in the first column of net_b.layer1.weight\n",
    "            new_connections_after_training = net_b.layer1.weight[:, 0]\n",
    "            print(\"new connections weights\", new_connections_after_training)\n",
    "            print(\"net a layer 1 weight: \", net_a.layer1.weight)\n",
    "            print(\"net b layer 1 weight: \", net_b.layer1.weight)\n",
    "            \n",
    "            # Check if at least one newly created edge has weights > threshold\n",
    "            if (new_connections_after_training > threshold).any():\n",
    "                # If there is interaction\n",
    "                \n",
    "                interaction_detected = True\n",
    "\n",
    "                # Change the input dim of net_a\n",
    "                input_dim = net_a.layer1.in_features - 1\n",
    "                output_dim = net_a.layer1.out_features\n",
    "                net_a.layer1 = nn.Linear(input_dim, output_dim)\n",
    "                \n",
    "                # 2) remove the predictor index from net_a_predictors\n",
    "                net_a_predictors.remove(predictor)\n",
    "\n",
    "                # Retrain net_a and net_b simultaneously until convergence\n",
    "                net_a, net_b = co_train(net_a=net_a, \n",
    "                                        net_b=net_b, \n",
    "                                        inputs=inputs, \n",
    "                                        targets=targets, \n",
    "                                        net_a_predictors=net_a_predictors,\n",
    "                                        net_b_predictors=net_b_predictors,\n",
    "                                        link_function=link_function,\n",
    "                                        num_epochs=num_epochs)\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # If there's no interaction:\n",
    "                \n",
    "                # 1) revert net_a and net_b to its original state\n",
    "                net_a = net_a_copy\n",
    "                net_b = net_b_copy\n",
    "                \n",
    "#                 input_dim = net_b.layer1.in_features - 1\n",
    "#                 output_dim = net_b.layer1.out_features\n",
    "#                 net_b.layer1 = nn.Linear(input_dim, output_dim)\n",
    "#                 net_b.layer1.weight = original_weight\n",
    "                \n",
    "                # 2) remove the new predictor index from net_b_predictors\n",
    "                net_b_predictors.remove(predictor)\n",
    "                \n",
    "        if not interaction_detected:\n",
    "            break\n",
    "\n",
    "    return net_a, net_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "12db034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(strength=1, size=1000):\n",
    "    '''\n",
    "    DGP based on the DAG in section IV\n",
    "    '''\n",
    "    \n",
    "    x1 = np.random.normal(0, 1, size=size)\n",
    "    x3 = np.random.normal(0, 1, size=size)\n",
    "    \n",
    "    x2 = strength*x1 + 10 * (np.random.normal(0, 1, size=size))**3 #0.1*strength*x1 + 3*np.random.normal(0, 1, size=size)\n",
    "    \n",
    "    u1 = strength*x2 + 3*x3 + np.random.normal(0, 1, size=size)\n",
    "    u2 = 5*x1 + np.random.normal(0, 1, size=size)\n",
    "\n",
    "    y = 3*u1 + u2**2 + np.exp(x3) + np.random.normal(0, 1, size=size)\n",
    "\n",
    "    return x1, x2, x3, y\n",
    "\n",
    "# y = strength*x2 + 3*x3 + np.random.normal(0, 1, size=size) + 5*x1 + np.random.normal(0, 1, size=size) + x3 + np.random.normal(0, 1, size=size)\n",
    "# y = (strength*0.3*strength + 5) * x1 + 4*x3 + np.random.normal(0, 1, size=size) + np.random.normal(0, 1, size=size) + np.random.normal(0, 1, size=size)\n",
    "\n",
    "    \n",
    "def get_data(sample_size, strength=1):\n",
    "    X1, X2, X3, Y = data_generator(strength=strength, size=sample_size)\n",
    "    Xmat = np.hstack((X1.reshape(-1, 1), X2.reshape(-1, 1), X3.reshape(-1, 1)))\n",
    "    return Xmat, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "84e7032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Training and Test Data\n",
    "sample_size = 10000\n",
    "Xmat, Y = get_data(sample_size, strength=2)\n",
    "Xmat_test, Y_test = get_data(sample_size, strength=2)\n",
    "\n",
    "# Convert the input features and target variable to tensors\n",
    "inputs = torch.tensor(Xmat, dtype=torch.float32)\n",
    "targets = torch.tensor(Y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "inputs_test = torch.tensor(Xmat_test, dtype=torch.float32)\n",
    "targets_test = torch.tensor(Y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "8220bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural networks net_a and net_b\n",
    "input_size_a = 2\n",
    "input_size_b = 1\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "\n",
    "net_a = NeuralNet(input_size_a, hidden_size, output_size)\n",
    "net_b = NeuralNet(input_size_b, hidden_size, output_size)\n",
    "\n",
    "# Define the link function as the identity function\n",
    "link_function = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "0338ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-train net_a and net_b\n",
    "net_a_predictors = [0, 2]\n",
    "net_b_predictors = [i for i in range(inputs.shape[1]) if i not in net_a_predictors]\n",
    "    \n",
    "net_a, net_b = co_train(net_a=net_a, \n",
    "                        net_b=net_b, \n",
    "                        inputs=inputs, \n",
    "                        targets=targets, \n",
    "                        net_a_predictors=net_a_predictors, \n",
    "                        net_b_predictors=net_b_predictors,\n",
    "                        link_function=link_function,\n",
    "                        num_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "d6863f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net b layer 1 weight:  Parameter containing:\n",
      "tensor([[-1.5175],\n",
      "        [-1.0549],\n",
      "        [ 1.4286],\n",
      "        [-1.9180],\n",
      "        [-1.1677],\n",
      "        [-0.8985],\n",
      "        [-0.4877],\n",
      "        [ 1.8959],\n",
      "        [ 1.5812],\n",
      "        [-1.5684]], requires_grad=True)\n",
      "\n",
      "layer1.weight tensor([[-1.5175],\n",
      "        [-1.0549],\n",
      "        [ 1.4286],\n",
      "        [-1.9180],\n",
      "        [-1.1677],\n",
      "        [-0.8985],\n",
      "        [-0.4877],\n",
      "        [ 1.8959],\n",
      "        [ 1.5812],\n",
      "        [-1.5684]])\n",
      "layer1.bias tensor([ 2.4323, -0.5087, -3.6505,  1.9652,  0.3322,  4.0892,  3.1666, -0.6447,\n",
      "        -2.7617,  1.8367])\n",
      "layer2.weight tensor([[-0.6852, -0.3825,  0.9988, -0.8597, -0.6320, -0.5116, -0.4012,  1.3917,\n",
      "          1.2194, -0.9714]])\n",
      "layer2.bias tensor([-0.9006])\n"
     ]
    }
   ],
   "source": [
    "# Get the weights of the neural network\n",
    "print(\"net b layer 1 weight: \", net_b.layer1.weight)\n",
    "print()\n",
    "for name, param in net_b.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "6efe14b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score for net_a: 0.02010162508761726\n",
      "R^2 score for net_b: 0.9512457839609639\n",
      "R^2 score for the full model: 0.997947279374086\n"
     ]
    }
   ],
   "source": [
    "n_features = inputs.shape[1]\n",
    "net_b_predictors = [i for i in range(n_features) if i not in net_a_predictors]\n",
    "\n",
    "# Get the outputs of net_a and net_b\n",
    "outputs_a = net_a(inputs_test[:, net_a_predictors]).detach().numpy()\n",
    "outputs_b = net_b(inputs_test[:, net_b_predictors]).detach().numpy()\n",
    "\n",
    "# Get the combined output of the full model\n",
    "combined_output = link_function(net_a(inputs_test[:, net_a_predictors]) + net_b(inputs_test[:, net_b_predictors])).detach().numpy()\n",
    "\n",
    "# Calculate R^2 scores\n",
    "r2_net_a = r2_score(targets_test.numpy(), outputs_a)\n",
    "r2_net_b = r2_score(targets_test.numpy(), outputs_b)\n",
    "r2_full_model = r2_score(targets_test.numpy(), combined_output)\n",
    "\n",
    "print(f\"R^2 score for net_a: {r2_net_a}\")\n",
    "print(f\"R^2 score for net_b: {r2_net_b}\")\n",
    "print(f\"R^2 score for the full model: {r2_full_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e2ed75f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new connections weights tensor([-0.2180, -0.1939, -0.1232], grad_fn=<SelectBackward0>)\n",
      "net a layer 1 weight:  Parameter containing:\n",
      "tensor([[ 0.0765, -3.6594],\n",
      "        [-7.2253,  1.8693],\n",
      "        [ 6.5555,  1.6924]], requires_grad=True)\n",
      "net b layer 1 weight:  Parameter containing:\n",
      "tensor([[-0.2180, -0.0037],\n",
      "        [-0.1939,  0.0141],\n",
      "        [-0.1232, -0.1274]], requires_grad=True)\n",
      "new connections weights tensor([ 0.0000, -0.1833, -0.3093], grad_fn=<SelectBackward0>)\n",
      "net a layer 1 weight:  Parameter containing:\n",
      "tensor([[ 0.0095, -3.9409],\n",
      "        [-7.1439,  1.7159],\n",
      "        [ 6.4215,  1.5799]], requires_grad=True)\n",
      "net b layer 1 weight:  Parameter containing:\n",
      "tensor([[ 0.0000,  0.0838],\n",
      "        [-0.1833,  0.8797],\n",
      "        [-0.3093, -0.9395]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(NeuralNet(\n",
       "   (layer1): Linear(in_features=2, out_features=3, bias=True)\n",
       "   (layer2): Linear(in_features=3, out_features=1, bias=True)\n",
       " ),\n",
       " NeuralNet(\n",
       "   (layer1): Linear(in_features=2, out_features=3, bias=True)\n",
       "   (layer2): Linear(in_features=3, out_features=1, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "largest_stable_subset(net_a=net_a, \n",
    "                      net_b=net_b, \n",
    "                      inputs=inputs, \n",
    "                      targets=targets, \n",
    "                      net_a_predictors=net_a_predictors, \n",
    "                      net_b_predictors=net_b_predictors, \n",
    "                      link_function=link_function, \n",
    "                      threshold=0.2, \n",
    "                      num_epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "97f29644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_b_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "2548061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "R^2 score for our method: 0.9335891298003772\n",
      "R^2 score for baseline 1: -20.54486319434331\n",
      "R^2 score for baseline 2: 0.8738260197651074\n"
     ]
    }
   ],
   "source": [
    "# Apply distribution shift\n",
    "# Get Training and Test Data\n",
    "Xmat_target, Y_target = get_data(50, strength=0.2)\n",
    "Xmat_test_target, Y_test_target = get_data(10000, strength=0.2)\n",
    "\n",
    "# Convert the input features and target variable to tensors\n",
    "inputs_target = torch.tensor(Xmat_target, dtype=torch.float32)\n",
    "targets_target = torch.tensor(Y_target, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "inputs_test_target = torch.tensor(Xmat_test_target, dtype=torch.float32)\n",
    "targets_test_target = torch.tensor(Y_test_target, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "### Our method ###\n",
    "net_a_target = copy.deepcopy(net_a)\n",
    "\n",
    "outputs_a_training = net_a_target(inputs_target[:, net_a_predictors]).detach().numpy()\n",
    "\n",
    "net_b_target = train_single_net(copy.deepcopy(net_b), \n",
    "                                inputs_target[:, net_b_predictors], \n",
    "                                targets_target - outputs_a_training, \n",
    "                                num_epochs=10000)\n",
    "\n",
    "\n",
    "# Get the outputs of net_a_target and net_b_target\n",
    "outputs_a = net_a_target(inputs_test_target[:, net_a_predictors]).detach().numpy()\n",
    "outputs_b = net_b_target(inputs_test_target[:, net_b_predictors]).detach().numpy()\n",
    "\n",
    "# Get the combined output of the full model\n",
    "combined_output = link_function(outputs_a + outputs_b)\n",
    "\n",
    "# a = outputs_a\n",
    "# c = combined_output\n",
    "\n",
    "r2_our_method = r2_score(targets_test_target.numpy(), combined_output)\n",
    "\n",
    "\n",
    "\n",
    "### Baseline 1: use source model directly on the target environment ###\n",
    "net_a_target = copy.deepcopy(net_a)\n",
    "net_b_target = copy.deepcopy(net_b)\n",
    "\n",
    "# Get the outputs of net_a_target and net_b_target\n",
    "outputs_a = net_a_target(inputs_test_target[:, net_a_predictors]).detach().numpy()\n",
    "outputs_b = net_b_target(inputs_test_target[:, net_b_predictors]).detach().numpy()\n",
    "\n",
    "# Get the combined output of the full model\n",
    "combined_output = link_function(outputs_a + outputs_b)\n",
    "\n",
    "r2_baseline_1 = r2_score(targets_test_target.numpy(), combined_output)\n",
    "\n",
    "print(len(inputs_target))\n",
    "\n",
    "### Baseline 2: directly retrain a new model ###\n",
    "\n",
    "# Create neural networks net_a and net_b\n",
    "input_size_a = 2\n",
    "input_size_b = 1\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "\n",
    "net_a_baseline_2 = NeuralNet(input_size_a, hidden_size, output_size)\n",
    "net_b_baseline_2 = NeuralNet(input_size_b, hidden_size, output_size)\n",
    "\n",
    "net_a_target, net_b_target = co_train(net_a=net_a_baseline_2, \n",
    "                                      net_b=net_b_baseline_2, \n",
    "                                      inputs=inputs_target, \n",
    "                                      targets=targets_target, \n",
    "                                      net_a_predictors=net_a_predictors, \n",
    "                                      net_b_predictors=net_b_predictors,\n",
    "                                      link_function=link_function,\n",
    "                                      num_epochs=12000)\n",
    "\n",
    "# Get the outputs of net_a_target and net_b_target\n",
    "outputs_a = net_a_target(inputs_test_target[:, net_a_predictors]).detach().numpy()\n",
    "outputs_b = net_b_target(inputs_test_target[:, net_b_predictors]).detach().numpy()\n",
    "\n",
    "# Get the combined output of the full model\n",
    "combined_output = link_function(outputs_a + outputs_b)\n",
    "r2_baseline_2 = r2_score(targets_test_target.numpy(), combined_output)\n",
    "\n",
    "# a2 = outputs_a\n",
    "# c2 = combined_output\n",
    "\n",
    "\n",
    "print(f\"R^2 score for our method: {r2_our_method}\")\n",
    "print(f\"R^2 score for baseline 1: {r2_baseline_1}\")\n",
    "print(f\"R^2 score for baseline 2: {r2_baseline_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "d5474b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 32.797195],\n",
       "       [ 46.679672],\n",
       "       [-16.292498],\n",
       "       ...,\n",
       "       [ 19.985012],\n",
       "       [ 21.911715],\n",
       "       [ 23.590187]], dtype=float32)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the strength of x1 -> x2 matters a lot, because if x1 is too predictive of x2,\n",
    "then the initial net_a and net_b might learn the alternative: really strong net_a that only uses x1, and a trash ent_b that doesn't really use x2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "2a6d66da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35.070965 ],\n",
       "       [73.17574  ],\n",
       "       [ 0.7389033],\n",
       "       ...,\n",
       "       [29.161621 ],\n",
       "       [26.557375 ],\n",
       "       [30.938162 ]], dtype=float32)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "8be64f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[68.49792   ],\n",
       "       [ 5.8692274 ],\n",
       "       [-0.48029232],\n",
       "       ...,\n",
       "       [15.224551  ],\n",
       "       [59.207523  ],\n",
       "       [94.538574  ]], dtype=float32)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "8b54b533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65.710106 ],\n",
       "       [ 3.846383 ],\n",
       "       [-3.4544182],\n",
       "       ...,\n",
       "       [ 9.74332  ],\n",
       "       [56.34586  ],\n",
       "       [97.97184  ]], dtype=float32)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "d4e38f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (layer1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (layer2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "net a layer 1 weight:  Parameter containing:\n",
      "tensor([[-2.3780,  0.0166],\n",
      "        [-1.2359,  1.7689],\n",
      "        [ 2.9769, -0.0293]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Apply distribution shift\n",
    "# Get Training and Test Data\n",
    "Xmat_target, Y_target = get_data(100, strength=1)\n",
    "Xmat_test_target, Y_test_target = get_data(10000, strength=1)\n",
    "\n",
    "# Convert the input features and target variable to tensors\n",
    "inputs_train = torch.tensor(Xmat_target, dtype=torch.float32)\n",
    "targets_train = torch.tensor(Y_target, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "inputs_test = torch.tensor(Xmat_test_target, dtype=torch.float32)\n",
    "targets_test = torch.tensor(Y_test_target, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(net_a)\n",
    "print(\"net a layer 1 weight: \", net_a.layer1.weight)\n",
    "\n",
    "### Our method ###\n",
    "net_a_target = copy.deepcopy(net_a)\n",
    "\n",
    "outputs_a_training = net_a_target(inputs_train[:, net_a_predictors]).detach().numpy()\n",
    "\n",
    "net_b_target = train_single_net(copy.deepcopy(net_b), \n",
    "                                inputs_train[:, net_b_predictors], \n",
    "                                targets_train - outputs_a_training, \n",
    "                                num_epochs=5000)\n",
    "\n",
    "\n",
    "# Get the outputs of net_a_target and net_b_target\n",
    "outputs_a = net_a_target(inputs_test[:, net_a_predictors]).detach().numpy()\n",
    "outputs_b = net_b_target(inputs_test[:, net_b_predictors]).detach().numpy()\n",
    "\n",
    "# Get the combined output of the full model\n",
    "combined_output = link_function(outputs_a + outputs_b)\n",
    "\n",
    "r2_our_method = r2_score(targets_test.numpy(), combined_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "76ee57aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8684668918672309"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_our_method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9142b5",
   "metadata": {},
   "source": [
    "## VII. Experiment 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55604723",
   "metadata": {},
   "source": [
    "questions:\n",
    "\n",
    "1. **If two variables do no interact, is it necessary that they are additive?**\n",
    "1. is it possible to prove that, if there are some edges into Y that are unstable and some edges into Y that are stable, then there must be no interaction between the unstable variables and the stable variables (cuz otherwise these \"stable\" variables would have been also unstable)? Can we then conclude that their relationship must be additive?\n",
    "\n",
    "GPT answer: \n",
    "It is not necessarily true that if there are some unstable edges into $Y$ and some stable edges into $Y$, there must be no interaction between the unstable variables and the stable variables. The stability of an edge depends on the consistency of the relationship between the variables across different environments. It is possible for variables with stable edges to interact with variables having unstable edges, but the nature of the interaction might be stable across environments, making the stable variables' edges remain stable.\n",
    "\n",
    "e.g.: Y = 3x + 4y + 5xy. If under shift, Y = 3x + 10y + 5xy, then x->Y is stable but y->Y is unstable, while x and y have interactions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ba9e9",
   "metadata": {},
   "source": [
    "## Appendix 1. Other Approaches That I Have Tried"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3866180",
   "metadata": {},
   "source": [
    "## Next steps:\n",
    "\n",
    "- Is it correct that the association learned by net_a and net_b are exactly through the association paths determined by d-speration? How can we prove it? PROVE IT: do some probabilities and drop the terms by indepedent (d-sep), eventually we will show that the association learned by net_a / net_b are exactly the things we wanted to learn. \n",
    "\n",
    "- More baseline models\n",
    "\n",
    "- Figure out the interaction terms thing.\n",
    "\n",
    "- Test on some real dataset\n",
    "\n",
    "### Summer:\n",
    "\n",
    "Maybe it's a good idea to take a step back and 1) learn causal inference thoroughly and systematically, and 2) read many papers related to domain adaptation and causal inference. Need to see what people are working on first. \n",
    "\n",
    "Ideally, find a topic that is 1) based on theory, not just emperical, and 2) have connects with this current piece of work.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Biggest Limitations of this research\n",
    "- we have to know the causal DAG\n",
    "- we have to require/assume that Y follows an additive Data Generating Process\n",
    "- we have to know which edge has shifted between two environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ebf67e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
